{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf98239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b25f891c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b68f62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"), # api_key required\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=1.2,\n",
    "    max_tokens=500 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c49ef50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eae11686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interview LangChain prompt format\n",
    "question_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert interviewer. Generate {num_questions} unique, clear, field-specific questions.\n",
    "\n",
    "Field: {field}\n",
    "Tone: {tone}\n",
    "                                                   \n",
    "Return format:\n",
    "1. Question 1\n",
    "2. Question 2\n",
    "3. Question 3                                                                                           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbbe4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Chain using LCEL\n",
    "question_chain = question_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67c21d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal Answer Generation Prompt\n",
    "ideal_answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert in {field}.\n",
    "Provide a concise, high-quality IDEAL answer for the interview question below.\n",
    "Question: {question}                                                     \n",
    "Return only the answer.                                \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e48f9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_answer_chain = ideal_answer_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "977e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¥ NEW: Evaluation Prompt\n",
    "evaluation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert interviewer and evaluator.\n",
    "\n",
    "Evaluate the candidate's interview based on the following questions and answers.\n",
    "\n",
    "Question: {question}\n",
    "Ideal Answer: {ideal_answer}\n",
    "User Answer: {user_answer}\n",
    "Similarity Score: {similarity_score}% \n",
    "\n",
    "                                                                               \n",
    "Write evaluation in this:\n",
    "                                                     \n",
    "format Score (0-10): X\n",
    "Feedback:\n",
    "- point 1\n",
    "- point 2\n",
    "\n",
    "Suggestions for Improvement:\n",
    "- point 1\n",
    "                                                                                                       \n",
    "Final Verdict (Good / Average / Poor):\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "685a7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New evaluation_chain using LCEL\n",
    "evaluation_chain = evaluation_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6de0f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_interview(field: str, tone: str, num_questions: int = 5):\n",
    "\n",
    "    # Step 1: Generate interview questions\n",
    "    questions_text = question_chain.invoke({\n",
    "        \"field\": field,\n",
    "        \"tone\": tone,\n",
    "        \"num_questions\": num_questions\n",
    "    })\n",
    "\n",
    "    questions = [q.strip() for q in questions_text.split(\"\\n\") if q.strip() and q.lstrip()[0].isdigit()]\n",
    "\n",
    "    print(f\"\\nðŸŽ¤ Starting {field} interview in a {tone} tone...\\n\")\n",
    "\n",
    "    # FAISS index for ideal answers\n",
    "    faiss_store = None\n",
    "    ideal_answer_texts = []\n",
    "\n",
    "    final_report = []\n",
    "    total_similarity = 0\n",
    "    total_score = 0\n",
    "\n",
    "    for i, q in enumerate(questions, 1):\n",
    "\n",
    "        print(f\"Q{i}: {q}\")\n",
    "        user_answer = input(\"ðŸ’­ Your answer: \").strip()\n",
    "\n",
    "        if user_answer.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nðŸ›‘ Interview ended by user.\")\n",
    "            break\n",
    "\n",
    "        # Step 2: Get ideal answer for this question\n",
    "        ideal_answer = ideal_answer_chain.invoke({\n",
    "            \"field\": field,\n",
    "            \"question\": q\n",
    "        })\n",
    "\n",
    "        ideal_answer_texts.append(ideal_answer)\n",
    "\n",
    "        # Step 3: Add ideal answer to FAISS store\n",
    "        if faiss_store is None:\n",
    "            faiss_store = FAISS.from_texts([ideal_answer], embeddings)\n",
    "        else:\n",
    "            faiss_store.add_texts([ideal_answer])\n",
    "\n",
    "        # Step 4: Use FAISS to get similarity\n",
    "        result = faiss_store.similarity_search_with_score(user_answer, k=1)\n",
    "        best_match, similarity_score = result[0]\n",
    "\n",
    "        # FAISS similarity score is distance â†’ convert to similarity %\n",
    "        similarity_percent = round((1 - similarity_score) * 100, 2)\n",
    "\n",
    "        total_similarity += similarity_percent\n",
    "\n",
    "        # Step 5: Evaluate\n",
    "        evaluation = evaluation_chain.invoke({\n",
    "            \"question\": q,\n",
    "            \"ideal_answer\": ideal_answer,\n",
    "            \"user_answer\": user_answer,\n",
    "            \"similarity_score\": similarity_percent\n",
    "        })\n",
    "\n",
    "        final_report.append({\n",
    "            \"question\": q,\n",
    "            \"similarity\": similarity_percent,\n",
    "            \"evaluation\": evaluation\n",
    "        })\n",
    "\n",
    "        score_match = re.search(r\"Score \\(0-10\\):\\s*(\\d+)\", evaluation)\n",
    "        if score_match:\n",
    "            total_score += int(score_match.group(1))\n",
    "\n",
    "    # -------------------------\n",
    "    # FINAL SUMMARY\n",
    "    # -------------------------\n",
    "\n",
    "    if len(final_report) == 0:\n",
    "        print(\"\\nâš  No answers provided â€” no summary generated.\\n\")\n",
    "        return final_report\n",
    "\n",
    "    print(\"\\n\\nðŸŽ¯ FINAL INTERVIEW SUMMARY\")\n",
    "    print(\"=======================================\\n\")\n",
    "\n",
    "    avg_similarity = round(total_similarity / len(final_report), 2)\n",
    "    avg_score = round(total_score / len(final_report), 2)\n",
    "\n",
    "    print(f\"â­ Average Similarity Score: {avg_similarity}%\")\n",
    "    print(f\"ðŸ† Average Interview Score: {avg_score}/10\")\n",
    "\n",
    "    if avg_score >= 8:\n",
    "        verdict = \"Excellent\"\n",
    "    elif avg_score >= 6:\n",
    "        verdict = \"Good\"\n",
    "    elif avg_score >= 4:\n",
    "        verdict = \"Average\"\n",
    "    else:\n",
    "        verdict = \"Poor\"\n",
    "\n",
    "    print(f\"\\nðŸ“Œ Final Verdict: {verdict}\")\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"ðŸ“˜ Detailed Per-Question Evaluation\\n\")\n",
    "\n",
    "    for i, item in enumerate(final_report, 1):\n",
    "        print(f\"Q{i}: {item['question']}\")\n",
    "        print(f\"Similarity: {item['similarity']}%\")\n",
    "        print(f\"Evaluation:\\n{item['evaluation']}\")\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Interview Completed Successfully!\")\n",
    "    return final_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f234b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¤ Starting Data Science interview in a friendly tone...\n",
      "\n",
      "Q1: 1. **Exploring Uncertainty**: You're tasked with developing a predictive model for a new product's sales forecast. However, there's limited historical data available. How would you approach the problem, and what data augmentation techniques would you use to improve the model's performance?\n",
      "\n",
      "ðŸ›‘ Interview ended by user.\n",
      "\n",
      "âš  No answers provided â€” no summary generated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run to start the interview\n",
    "if __name__ == \"__main__\":\n",
    "    start_interview(field=\"Data Science\", tone=\"friendly\",\n",
    "num_questions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe318c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
