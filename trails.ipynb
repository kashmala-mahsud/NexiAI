{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf98239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yoush\\NexiAI\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import statements\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25f891c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68f62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"), # api_key required\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=1.2,\n",
    "    max_tokens=500 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49ef50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yoush\\AppData\\Local\\Temp\\ipykernel_2060\\1112972915.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Embeddings Model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eae11686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interview LangChain prompt format\n",
    "question_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert interviewer. Generate {num_questions} unique, clear, field-specific questions.\n",
    "\n",
    "Field: {field}\n",
    "Tone: {tone}\n",
    "                                                   \n",
    "Return format:\n",
    "1. Question 1\n",
    "2. Question 2\n",
    "3. Question 3                                                                                           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbbe4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Chain using LCEL\n",
    "question_chain = question_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c21d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal Answer Generation Prompt\n",
    "ideal_answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert in {field}.\n",
    "Provide a concise, high-quality IDEAL answer for the interview question below.\n",
    "                                                      Question: {question}                                                     \n",
    "Return only the answer.                                \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e48f9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_answer_chain = ideal_answer_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¥ NEW: Evaluation Prompt\n",
    "evaluation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert interviewer and evaluator.\n",
    "\n",
    "Evaluate the candidate's interview based on the following questions and answers.\n",
    "\n",
    "Question: {question}\n",
    "Ideal Answer: {ideal_answer}\n",
    "User Answer: {user_answer}\n",
    "Similarity Score: {similarity_score}% \n",
    "\n",
    "                                                                               \n",
    "Write evaluation in this:\n",
    "                                                     \n",
    "format Score (0-10): X\n",
    "Feedback:\n",
    "- point 1\n",
    "- point 2\n",
    "\n",
    "Suggestions for Improvement:\n",
    "- point 1\n",
    "                                                                                                       \n",
    "Final Verdict (Good / Average / Poor):\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685a7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New evaluation_chain using LCEL\n",
    "evaluation_chain = evaluation_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_interview(field: str, tone: str, num_questions: int = 5):\n",
    "    questions_text = question_chain.invoke({\n",
    "        \"field\": field,\n",
    "        \"tone\": tone,\n",
    "        \"num_questions\": num_questions\n",
    "    })\n",
    "\n",
    "    questions = [q.strip() for q in questions_text.split(\"\\n\") if q.strip() and q.lstrip()[0].isdigit()]\n",
    "\n",
    "    print(f\"\\nðŸŽ¤ Starting {field} interview in a {tone} tone...\\n\")\n",
    "\n",
    "    final_report = []          # store per-question evaluation\n",
    "    total_similarity = 0       # for average score\n",
    "    total_score = 0            # from AI evaluations (0â€“10 scale)\n",
    "\n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"Q{i}: {q}\")\n",
    "        user_answer = input(\"ðŸ’­ Your answer: \").strip()\n",
    "\n",
    "        if user_answer.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nðŸ›‘ Interview ended by user.\")\n",
    "            break\n",
    "\n",
    "        ideal_answer = ideal_answer_chain.invoke({\n",
    "            \"field\": field,\n",
    "            \"question\": q\n",
    "        })\n",
    "\n",
    "        # Embeddings\n",
    "        docs = [ideal_answer, user_answer]\n",
    "        vectors = [embeddings.embed_query(d) for d in docs]\n",
    "        similarity = cosine_similarity(vectors[0], vectors[1])\n",
    "        similarity_percent = round(similarity * 100, 2)\n",
    "\n",
    "        # Store accumulated sim\n",
    "        total_similarity += similarity_percent\n",
    "\n",
    "        # AI Evaluation\n",
    "        evaluation = evaluation_chain.invoke({\n",
    "            \"question\": q,\n",
    "            \"ideal_answer\": ideal_answer,\n",
    "            \"user_answer\": user_answer,\n",
    "            \"similarity_score\": similarity_percent\n",
    "        })\n",
    "\n",
    "        final_report.append({\n",
    "            \"question\": q,\n",
    "            \"similarity\": similarity_percent,\n",
    "            \"evaluation\": evaluation\n",
    "        })\n",
    "\n",
    "        # Extract numeric score from evaluation text\n",
    "        import re\n",
    "        score_match = re.search(r\"Score \\(0-10\\):\\s*(\\d+)\", evaluation)\n",
    "        if score_match:\n",
    "            total_score += int(score_match.group(1))\n",
    "\n",
    "    # -------------------------\n",
    "    # FINAL SUMMARY REPORT\n",
    "    # -------------------------\n",
    "\n",
    "    if len(final_report) == 0:\n",
    "        print(\"\\nâš  Interview ended before any answer was provided.\")\n",
    "        print(\"No evaluation summary available.\\n\")\n",
    "        return final_report\n",
    "\n",
    "    print(\"\\n\\nðŸŽ¯ FINAL INTERVIEW SUMMARY\")\n",
    "    print(\"=======================================\\n\")\n",
    "\n",
    "    avg_similarity = round(total_similarity / len(final_report), 2)\n",
    "    avg_score = round(total_score / len(final_report), 2)\n",
    "\n",
    "    print(f\"â­ Average Similarity Score: {avg_similarity}%\")\n",
    "    print(f\"ðŸ† Average Interview Score: {avg_score}/10\")\n",
    "\n",
    "    if avg_score >= 8:\n",
    "        verdict = \"Excellent\"\n",
    "    elif avg_score >= 6:\n",
    "        verdict = \"Good\"\n",
    "    elif avg_score >= 4:\n",
    "        verdict = \"Average\"\n",
    "    else:\n",
    "        verdict = \"Poor\"\n",
    "\n",
    "    print(f\"\\nðŸ“Œ Final Verdict: {verdict}\")\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(\"ðŸ“˜ Detailed Per-Question Evaluation\\n\")\n",
    "\n",
    "    for i, item in enumerate(final_report, 1):\n",
    "        print(f\"Q{i}: {item['question']}\")\n",
    "        print(f\"Similarity: {item['similarity']}%\")\n",
    "        print(f\"Evaluation:\\n{item['evaluation']}\")\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Interview Completed Successfully!\")\n",
    "    return final_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f234b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¤ Starting Data Science interview in a friendly tone...\n",
      "\n",
      "Q1: Explain bias-variance tradeoff in ML.\n",
      "\n",
      "ðŸ›‘ Interview ended by user.\n",
      "\n",
      "âš  Interview ended before any answer was provided.\n"
     ]
    }
   ],
   "source": [
    "# Run to start the interview\n",
    "if __name__ == \"__main__\":\n",
    "    start_interview(field=\"Data Science\", tone=\"friendly\",\n",
    "num_questions=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
